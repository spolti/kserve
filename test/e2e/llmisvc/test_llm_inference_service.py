# Copyright 2025 The KServe Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import time
from dataclasses import dataclass
from typing import Any, Callable, List
import pytest
import requests
from kubernetes import client
import yaml
from kserve import KServeClient, V1alpha1LLMInferenceService, constants
from .diagnostic import (
    print_all_events_table,
    kinds_matching_by_labels,
)
from .fixtures import (
    generate_test_id,
    # Factory functions are not called explicitly, but they need to be imported to work
    test_case,  # noqa: F401,F811
)
from .logging import log_execution

KSERVE_PLURAL_LLMINFERENCESERVICE = "llminferenceservices"


def assert_200(response: requests.Response) -> None:
    """Default response assertion that checks for 200 status code."""
    assert (
        response.status_code == 200
    ), f"Service returned {response.status_code}: {response.text}"


@dataclass
class TestCase:
    __test__ = False  # So pytest will not try to execute it.
    """Test case configuration for LLM inference service tests."""
    base_refs: List[str]
    prompt: str
    max_tokens: int = 10
    response_assertion: Callable[[requests.Response], None] = assert_200
    wait_timeout: int = 300
    response_timeout: int = 60
    # Factory provided
    llm_service: V1alpha1LLMInferenceService = None  # Generated by llm_service_factory
    model_name: str = "default/model"  # This will be generated by the factory


@pytest.mark.llminferenceservice
@pytest.mark.asyncio(loop_scope="session")
@pytest.mark.parametrize(
    "test_case",
    [
        pytest.param(
            TestCase(
                base_refs=["router-managed", "workload-single-cpu", "model-fb-opt-125m"],
                prompt="KServe is a",
            ),
            marks=[pytest.mark.cluster_cpu, pytest.mark.cluster_single_node],
        ),
        pytest.param(
            TestCase(
                base_refs=["router-managed", "workload-pd-cpu", "model-fb-opt-125m"],
                prompt="You are an expert in Kubernetes-native machine learning serving platforms, with deep knowledge of the KServe project. "
                       "Explain the challenges of serving large-scale models, GPU scheduling, and how KServe integrates with capabilities like multi-model serving. "
                       "Provide a detailed comparison with open source alternatives, focusing on operational trade-offs.",
                response_assertion=lambda response: (
                        response.status_code == 200
                        and response.json().get("choices") is not None
                        and len(response.json().get("choices", [])) > 0
                ),
            ),
            marks=[pytest.mark.cluster_cpu, pytest.mark.cluster_single_node],
        ),
    ],
    indirect=["test_case"],
    ids=generate_test_id,
)
@log_execution
def test_llm_inference_service(test_case: TestCase):

    kserve_client = KServeClient(
        config_file=os.environ.get("KUBECONFIG", "~/.kube/config")
    )

    service_name = test_case.llm_service.metadata.name

    try:
        create_llmisvc(kserve_client, test_case.llm_service)
        wait_for_model_response(kserve_client, test_case, test_case.wait_timeout)
    except Exception as e:
        print(f"‚ùå ERROR: Failed to call llm inference service {service_name}: {e}")
        _collect_diagnostics(kserve_client, test_case.llm_service)
        raise
    finally:
        try:
            delete_llmisvc(kserve_client, test_case.llm_service)
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Failed to cleanup service {service_name}: {e}")


@log_execution
def create_llmisvc(kserve_client: KServeClient, llm_isvc: V1alpha1LLMInferenceService):
    try:
        outputs = kserve_client.api_instance.create_namespaced_custom_object(
            constants.KSERVE_GROUP,
            llm_isvc.api_version.split("/")[1],
            llm_isvc.metadata.namespace,
            KSERVE_PLURAL_LLMINFERENCESERVICE,
            llm_isvc,
        )
        print(f"‚úÖ LLM inference service {llm_isvc.metadata.name} created successfully")
        return outputs
    except client.rest.ApiException as e:
        raise RuntimeError(
            f"‚ùå Exception when calling CustomObjectsApi->"
            f"create_namespaced_custom_object for LLMInferenceService: {e}"
        ) from e


@log_execution
def delete_llmisvc(kserve_client: KServeClient, llm_isvc: V1alpha1LLMInferenceService):
    try:
        result = kserve_client.api_instance.delete_namespaced_custom_object(
            constants.KSERVE_GROUP,
            llm_isvc.api_version.split("/")[1],
            llm_isvc.metadata.namespace,
            KSERVE_PLURAL_LLMINFERENCESERVICE,
            llm_isvc.metadata.name,
        )
        print(f"‚úÖ LLM inference service {llm_isvc.metadata.name} deleted successfully")
        return result
    except client.rest.ApiException as e:
        raise RuntimeError(
            f"‚ùå Exception when calling CustomObjectsApi->"
            f"delete_namespaced_custom_object for LLMInferenceService: {e}"
        ) from e


@log_execution
def get_llmisvc(
    kserve_client: KServeClient,
    name,
    namespace,
    version=constants.KSERVE_V1ALPHA1_VERSION,
):
    try:
        return kserve_client.api_instance.get_namespaced_custom_object(
            constants.KSERVE_GROUP,
            version,
            namespace,
            KSERVE_PLURAL_LLMINFERENCESERVICE,
            name,
        )
    except client.rest.ApiException as e:
        raise RuntimeError(
            f"‚ùå Exception when calling CustomObjectsApi->"
            f"get_namespaced_custom_object for LLMInferenceService: {e}"
        ) from e


@log_execution
def wait_for_model_response(
    kserve_client: KServeClient,
    test_case: TestCase,
    timeout_seconds: int = 600,
) -> str:

    service_url = None

    def assert_model_responds():
        nonlocal service_url

        try:
            service_url = get_llm_service_url(kserve_client, test_case.llm_service)
        except Exception as e:
            raise AssertionError(f"‚ùå Failed to get service URL: {e}") from e

        completion_url = f"{service_url}/v1/completions"
        test_payload = {
            "model": test_case.model_name,
            "prompt": test_case.prompt,
            "max_tokens": test_case.max_tokens,
        }
        print(f"Calling LLM service at {completion_url} with payload {test_payload}")
        try:
            response = requests.post(
                completion_url,
                headers={"Content-Type": "application/json"},
                json=test_payload,
                timeout=test_case.response_timeout,
            )
        except Exception as e:
            raise AssertionError(f"‚ùå Failed to call model: {e}") from e

        test_case.response_assertion(response)
        print("‚úÖ LLM service responded successfully")
        return service_url

    return wait_for(assert_model_responds, timeout=timeout_seconds, interval=10.0)


def get_llm_service_url(
    kserve_client: KServeClient, llm_isvc: V1alpha1LLMInferenceService
):
    service_name = llm_isvc.metadata.name

    try:
        llm_isvc = get_llmisvc(
            kserve_client,
            llm_isvc.metadata.name,
            llm_isvc.metadata.namespace,
            llm_isvc.api_version.split("/")[1],
        )

        if "status" not in llm_isvc:
            raise ValueError(
                f"‚ùå No status found in LLM inference service {service_name} status: {llm_isvc}"
            )

        status = llm_isvc["status"]

        if "url" in status and status["url"]:
            return status["url"]

        if (
            "addresses" in status
            and status["addresses"]
            and len(status["addresses"]) > 0
        ):
            first_address = status["addresses"][0]
            if "url" in first_address:
                return first_address["url"]

        raise ValueError(
            f"‚ùå No URL found in LLM inference service {service_name} status"
        )

    except Exception as e:
        raise ValueError(
            f"‚ùå Failed to get URL for LLM inference service {service_name}: {e}"
        ) from e


def wait_for(
    assertion_fn: Callable[[], Any], timeout: float = 5.0, interval: float = 0.1
) -> Any:
    """Wait for the assertion to succeed within timeout."""
    deadline = time.time() + timeout
    while True:
        try:
            return assertion_fn()
        except AssertionError:
            if time.time() >= deadline:
                raise
            time.sleep(interval)


def _collect_diagnostics(
    kserve_client: KServeClient, llm_isvc: V1alpha1LLMInferenceService
):
    name = llm_isvc.metadata.name
    ns = llm_isvc.metadata.namespace

    svc = get_llmisvc(kserve_client, name, ns)

    labels = {
        "app.kubernetes.io/part-of": "llminferenceservice",
        "app.kubernetes.io/name": name,
    }

    print(f"üîç # Diagnostics for {name!r} in {ns!r}")
    print("---")
    print(f"# LLMInferenceService {name}")
    try:
        print(yaml.safe_dump(svc, sort_keys=False))
    except Exception as e:
        print(f"# ‚ùå failed to dump LLMInferenceService: {e}")

    print_all_events_table(ns)

    all_resources = kinds_matching_by_labels(ns, labels)
    for obj in all_resources:
        print("---")
        print(yaml.safe_dump(obj.to_dict(), sort_keys=False))
